{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjuo49IN2Vgj",
        "outputId": "31425715-a7ad-4bff-e470-f893d9aa6858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 167s 16ms/step - loss: 0.5295 - accuracy: 0.7385 - val_loss: 0.4670 - val_accuracy: 0.7769 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 153s 15ms/step - loss: 0.4962 - accuracy: 0.7628 - val_loss: 0.4572 - val_accuracy: 0.7831 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 153s 15ms/step - loss: 0.4873 - accuracy: 0.7672 - val_loss: 0.4514 - val_accuracy: 0.7868 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 153s 15ms/step - loss: 0.4817 - accuracy: 0.7702 - val_loss: 0.4514 - val_accuracy: 0.7884 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 163s 16ms/step - loss: 0.4779 - accuracy: 0.7722 - val_loss: 0.4484 - val_accuracy: 0.7890 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 163s 16ms/step - loss: 0.4749 - accuracy: 0.7737 - val_loss: 0.4450 - val_accuracy: 0.7908 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 153s 15ms/step - loss: 0.4728 - accuracy: 0.7749 - val_loss: 0.4457 - val_accuracy: 0.7907 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 152s 15ms/step - loss: 0.4710 - accuracy: 0.7762 - val_loss: 0.4425 - val_accuracy: 0.7919 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 152s 15ms/step - loss: 0.4693 - accuracy: 0.7770 - val_loss: 0.4453 - val_accuracy: 0.7925 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 152s 15ms/step - loss: 0.4680 - accuracy: 0.7773 - val_loss: 0.4430 - val_accuracy: 0.7925 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 153s 15ms/step - loss: 0.4668 - accuracy: 0.7780 - val_loss: 0.4415 - val_accuracy: 0.7924 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 152s 15ms/step - loss: 0.4663 - accuracy: 0.7785 - val_loss: 0.4410 - val_accuracy: 0.7942 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 154s 15ms/step - loss: 0.4655 - accuracy: 0.7788 - val_loss: 0.4384 - val_accuracy: 0.7945 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 153s 15ms/step - loss: 0.4647 - accuracy: 0.7793 - val_loss: 0.4426 - val_accuracy: 0.7932 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 152s 15ms/step - loss: 0.4640 - accuracy: 0.7797 - val_loss: 0.4377 - val_accuracy: 0.7947 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 153s 15ms/step - loss: 0.4633 - accuracy: 0.7798 - val_loss: 0.4375 - val_accuracy: 0.7957 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 153s 15ms/step - loss: 0.4629 - accuracy: 0.7802 - val_loss: 0.4358 - val_accuracy: 0.7956 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 152s 15ms/step - loss: 0.4626 - accuracy: 0.7808 - val_loss: 0.4362 - val_accuracy: 0.7956 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 152s 15ms/step - loss: 0.4620 - accuracy: 0.7808 - val_loss: 0.4357 - val_accuracy: 0.7962 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 152s 15ms/step - loss: 0.4618 - accuracy: 0.7809 - val_loss: 0.4358 - val_accuracy: 0.7957 - lr: 0.0010\n",
            "10000/10000 [==============================] - 38s 4ms/step - loss: 0.4358 - accuracy: 0.7957\n",
            "Test Accuracy: 0.7956968545913696\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "Sentence: feeling really sick today\n",
            "Predicted Sentiment: 0, True Sentiment: 0\n",
            "\n",
            "Sentence: also hope didnt freak alice wasnt intentional\n",
            "Predicted Sentiment: 1, True Sentiment: 1\n",
            "\n",
            "Sentence: ledoug actually planning working wth ala meeting kaiboshed plan\n",
            "Predicted Sentiment: 0, True Sentiment: 1\n",
            "\n",
            "Sentence: hey xonline nooo miss rob xlive live gt\n",
            "Predicted Sentiment: 1, True Sentiment: 1\n",
            "\n",
            "Sentence: voodoo met really cool person bus stop lol\n",
            "Predicted Sentiment: 0, True Sentiment: 0\n",
            "\n",
            "Sentence: already dreading going back work tuesday\n",
            "Predicted Sentiment: 0, True Sentiment: 0\n",
            "\n",
            "Sentence: sad everyone left houston\n",
            "Predicted Sentiment: 1, True Sentiment: 0\n",
            "\n",
            "Sentence: audreypanda alright feel better get better soon maybe caught swine flu marmar lol\n",
            "Predicted Sentiment: 0, True Sentiment: 0\n",
            "\n",
            "Sentence: computer spazzing\n",
            "Predicted Sentiment: 1, True Sentiment: 1\n",
            "\n",
            "Sentence: kspidel hopefully fix soon video tonight pretty useless right\n",
            "Predicted Sentiment: 0, True Sentiment: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization, SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import random\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the dataset\n",
        "columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
        "df = pd.read_csv('/content/drive/MyDrive/sentiment140/training.1600000.processed.noemoticon.csv', encoding='latin-1', names=columns)\n",
        "\n",
        "# Select only the 'sentiment' and 'text' columns\n",
        "df = df[['sentiment', 'text']]\n",
        "\n",
        "# Lowercasing\n",
        "df['text'] = df['text'].str.lower()\n",
        "\n",
        "# Removing URLs\n",
        "df['text'] = df['text'].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', '', x))\n",
        "\n",
        "# Removing special characters and numbers\n",
        "df['text'] = df['text'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
        "\n",
        "# Tokenization\n",
        "df['text'] = df['text'].apply(word_tokenize)\n",
        "\n",
        "# Removing stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['text'] = df['text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "\n",
        "# Combining tokens back into sentences\n",
        "df['text'] = df['text'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Tokenize the preprocessed text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "X = tokenizer.texts_to_sequences(df['text'])\n",
        "X = pad_sequences(X, maxlen=100)\n",
        "\n",
        "# Train/test split\n",
        "df['sentiment'] = df['sentiment'].map(lambda x: 1 if x == 4 else 0)\n",
        "y = df['sentiment'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=[sentence.split() for sentence in df['text']], vector_size=100, window=5, min_count=1, workers=4)\n",
        "word2vec_model.train([sentence.split() for sentence in df['text']], total_examples=word2vec_model.corpus_count, epochs=10)\n",
        "\n",
        "# Create embedding matrix\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, weights=[embedding_matrix], input_length=100, trainable=False))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv1D(filters=256, kernel_size=4, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv1D(filters=256, kernel_size=5, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=128, validation_data=(X_test, y_test), callbacks=[reduce_lr, early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Select 10 random sentences from the test dataset\n",
        "random_indices = random.sample(range(X_test.shape[0]), 10)\n",
        "X_sample = X_test[random_indices]\n",
        "y_sample = y_test[random_indices]\n",
        "\n",
        "# Predict the labels for the selected sentences\n",
        "y_pred_sample = model.predict(X_sample)\n",
        "\n",
        "# Print the results\n",
        "for i in range(10):\n",
        "    print(f\"Sentence: {df['text'].iloc[random_indices[i]]}\")\n",
        "    print(f\"Predicted Sentiment: {round(y_pred_sample[i][0])}, True Sentiment: {y_sample[i]}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}